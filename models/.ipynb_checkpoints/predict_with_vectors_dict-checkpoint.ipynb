{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Citations with Word Vectors\n",
    "Here we try to predict whether a sentence needs a 'citation needed' tag or not based on\n",
    "1. Word vectors only (english, all languages)\n",
    "2. Word vectors + main sec indicator\n",
    "3. Word vectors + article position indicator\n",
    "\n",
    "\n",
    "### Dependencies\n",
    "In order to perform some of the feature extraction tasks, there are some prerequisites for that.\n",
    "- treetagger (in order to install it, please follow the guidelines in http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/) \n",
    "- install the treetagger wraper for python (pip install treetaggerwrapper http://treetaggerwrapper.readthedocs.io/en/latest/)\n",
    "- download the models for English, Italian, French from http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "import hashlib\n",
    "import treetaggerwrapper\n",
    "import sys\n",
    "\n",
    "reload(sys)  \n",
    "sys.setdefaultencoding('utf8')\n",
    "\n",
    "DEPTH_SEARCH=[5,10,30, 50, 100, 200]\n",
    "NTREES_SEARCH=[5,10,30, 50, 100, 200]\n",
    "TEST_SIZE=0.33\n",
    "\n",
    "# POS tagggers\n",
    "TAGGERS = {'en': treetaggerwrapper.TreeTagger(TAGLANG='en'),\n",
    "           'fr': treetaggerwrapper.TreeTagger(TAGLANG='fr')\n",
    "           'it': treetaggerwrapper.TreeTagger(TAGLANG='it')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check if a lexicon entry appears in a given sentence (that is POS annotated). In some cases the dictionaries consists of verbs only, or other non-specific POS\n",
    "lexicon_check = lambda x, tags, filter_val: x in [token[0] for token in tags if token[1].startswith(filter_val)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Reads a lexicon where each line contains a single word\n",
    "'''\n",
    "def read_lexicon(file_name, filter_key='#'):\n",
    "    f = open(file_name, 'r')\n",
    "    lexicon = [line.strip() for line in f if\n",
    "               not line.startswith(filter_key) and len(line.strip()) != 0 and \" \" not in line.strip()]\n",
    "    f.close()\n",
    "    return lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Check for a statement if it contains entries from a predefined dictionary. \n",
    "    It returns a False/True value, we can tweak this such that it can return a ratio or frequency.\n",
    "'''\n",
    "def annotate_line_with_dict_entries(lexicon, statement_pos_tags, pos_tag_filter):\n",
    "    # tokenize first the sentence and annotate with POS\n",
    "    has_entry = any([lexicon_check(x, statement_pos_tags, pos_tag_filter) for x in lexicon])\n",
    "    return has_entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Annotate a sentence with POS tags and additionally lematize the tokens.\n",
    "'''\n",
    "def tag_statement(lang, statement):\n",
    "    tagger = TAGGERS[lang]\n",
    "    #annotate the statement with tags, which consist of the \"word\\tPOS\\tlemma\".\n",
    "    tags = tagger.tag_text(statement)\n",
    "    #parse them into a more structured representation, where each entry consists  word=u'is', pos=u'VBZ', lemma=u'be'\n",
    "\n",
    "    parsed_tags = treetaggerwrapper.make_tags(tags)\n",
    "    return parsed_tags\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filenames={}\n",
    "filenames_raw={}\n",
    "\n",
    "\"\"\"\n",
    "to use this, you will need: \n",
    "1) features from https://drive.google.com/open?id=1JZu67psmj2Eou2-8wQEJk4kAQfg8GDs2, to be placed in ../fastText_multilingual/features\n",
    "\"\"\"\n",
    "languages=['en']\n",
    "language_extended=['english']\n",
    "feadir='../fastText_multilingual/features/'\n",
    "rawdir='../data_clean/'\n",
    "\n",
    "def load_languages():\n",
    "    for lan,lext in zip(languages,language_extended):\n",
    "        filenames[lan]=feadir+lan+'.tsv' #files with vectors\n",
    "        filenames_raw[lan]=rawdir+lext+'.tsv' #files with raw text\n",
    "\n",
    "def count_negatives(negatives,positives):\n",
    "    \"\"\"\n",
    "    for balanced data, we need to know how many negatives are out there\n",
    "    \"\"\"\n",
    "    proportion={}\n",
    "    allneg=0\n",
    "    for lan in languages:\n",
    "        proportion[lan]=len(negatives[lan])/float(len(negatives[lan])+len(positives[lan]))\n",
    "        allneg+=len(negatives[lan])\n",
    "    print 'proportion of negatives per language'\n",
    "    print proportion\n",
    "    return allneg\n",
    "\n",
    "def get_values_for_crossvalidation(positives,negatives,features):\n",
    "    \"\"\"\n",
    "    positives: list of positives\n",
    "    negatives: list of negatives\n",
    "    features: list of feature dictionaries, per type\n",
    "    \"\"\"\n",
    "    values=[]\n",
    "    y=[]\n",
    "    ids=[]\n",
    "    for lan in languages:\n",
    "        shuffle(positives[lan])\n",
    "        alldata=set(negatives[lan]+positives[lan][:len(negatives[lan])])\n",
    "        ids=ids+list(alldata)\n",
    "        for id in alldata:\n",
    "            v=[]\n",
    "            for f in features: #for every type of feature\n",
    "                if isinstance(f[id], int):\n",
    "                    v.append(f[id])\n",
    "                else:\n",
    "                    for element in f[id]: #append element of feature\n",
    "                        v.append(element)\n",
    "            values.append(np.nan_to_num(np.asarray(v)))\n",
    "            y.append(labels[id])          \n",
    "    #reshuffle everything for cross_validaton\n",
    "    ind=range(len(y))\n",
    "    shuffle(ind)\n",
    "    y2=[y[i] for i in ind]\n",
    "    values2=[values[i] for i in ind]\n",
    "    ids2=[ids[i] for i in ind]\n",
    "    return y2,values2,ids2\n",
    "\n",
    "def perform_gridsearch_withRFC(values,y):\n",
    "    \"\"\"\n",
    "    values: list of feature vectors\n",
    "    y: labels\n",
    "    returns\n",
    "    max_ind: depth and estimator values\n",
    "    max_val: crossval prediction accuracy\n",
    "    scores: all-scores for each combination of depth and nestimators\n",
    "    \"\"\"\n",
    "    scores={}\n",
    "    #performs cross_validation in all combiantions\n",
    "    for d in DEPTH_SEARCH:\n",
    "        for n in NTREES_SEARCH:\n",
    "            clf = RandomForestClassifier(max_depth=d, n_estimators=n)\n",
    "            s = cross_val_score(clf, values, y)\n",
    "            print s\n",
    "            scores[str(d)+' '+str(n)]=np.mean(s)\n",
    "    #computes best combination of parameters\n",
    "    max_ind=''\n",
    "    max_val=0\n",
    "    for s in scores:\n",
    "        if scores[s]>max_val:\n",
    "            max_val=scores[s]\n",
    "            max_ind=s\n",
    "    print max_ind\n",
    "    print max_val\n",
    "    return max_ind,max_val,scores\n",
    "\n",
    "def train_test_final(val_train,val_test,y_train,d,n):\n",
    "    \"\"\"\n",
    "    just using a Random Forestc classifier on a train/test split for deployment \n",
    "    returns model and probability on the test set\n",
    "    \"\"\"\n",
    "    clf = RandomForestClassifier(max_depth=d, n_estimators=n)\n",
    "    clf.fit(val_train,y_train)\n",
    "    prob=clf.predict_proba(val_test)\n",
    "    return clf,prob\n",
    "\n",
    "def print_top_bottom_sentences(prob,ids_test,y_test,text,labels):\n",
    "    \"\"\"\n",
    "    here we are displaying the \n",
    "    \"\"\"\n",
    "    pos_proba=(np.asarray(prob).T)[1]\n",
    "    indexes=np.argsort(-np.asarray(pos_proba))\n",
    "    for i in indexes[:10]:\n",
    "        print text[ids_test[i]]\n",
    "        print y_test[i]\n",
    "        print labels[ids_test[i]]#checking\n",
    "    print ('********************************')\n",
    "    for i in indexes[-10:]:\n",
    "        print text[ids_test[i]]\n",
    "        print y_test[i]\n",
    "        print pos_proba[i]\n",
    "        print labels[ids_test[i]]#checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load labels and vectors.. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "load_languages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Load all the dictionaries we use for classifying the statements. At the moment we have dictionaries only in english.\n",
    "    We will translate the dictionaries into French and Italian.\n",
    "'''\n",
    "dict_path = '../data/dictionaries/'\n",
    "lx_implicative_verbs = read_lexicon(dict_path + '/implicatives_karttunen1971.txt')\n",
    "lx_report_verbs = read_lexicon(dict_path + '/report_verbs.txt')\n",
    "lx_factive_verbs = read_lexicon(dict_path + '/factives_hooper1975.txt')\n",
    "lx_hedges = read_lexicon(dict_path + '/hedges_hyland2005.txt')\n",
    "lx_assertive_verbs = read_lexicon(dict_path + '/assertives_hooper1975.txt')\n",
    "\n",
    "print 'Loaded the lexical dictionaries.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load all data for prediction into different variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-f37639cd5ca5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mreport\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mannotate_line_with_dict_entries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlx_report_verbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement_pos_tags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'V'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mhedges\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mannotate_line_with_dict_entries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlx_hedges\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement_pos_tags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0massertive\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mannotate_line_with_dict_entries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlx_assertive_verbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement_pos_tags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'V'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mlanguage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-72a921547eb4>\u001b[0m in \u001b[0;36mannotate_line_with_dict_entries\u001b[0;34m(lexicon, statement_pos_tags, pos_tag_filter)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mannotate_line_with_dict_entries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlexicon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement_pos_tags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_tag_filter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# tokenize first the sentence and annotate with POS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mhas_entry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlexicon_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement_pos_tags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_tag_filter\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlexicon\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhas_entry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-81cc74ce2aee>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x, tags, filter_val)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# check if a lexicon entry appears in a given sentence (that is POS annotated). In some cases the dictionaries consists of verbs only, or other non-specific POS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlexicon_check\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_val\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtags\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python2.7/encodings/utf_8.pyc\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(input, errors)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mencode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutf_8_encode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'strict'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutf_8_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "raw header is:\n",
    "entity_id\trevision_id\ttimestamp entity_title\tsection\tstart\toffset\tstatement label\n",
    "feature header is:\n",
    "entity_id\trevision_id\ttimestamp entity_title\tsection\tstart\toffset\t label feature\n",
    "\"\"\"\n",
    "labels={} #whether it needs a citation or not\n",
    "vectors={} #the word vectors aligned to english\n",
    "main={} #is it the main section?\n",
    "factive={} # does the statement contain a factive verb?\n",
    "implicative={} # does the statement contain an implicative verb?\n",
    "hedges={} # does the statement contain hedges?\n",
    "assertive={} # does the statement contain any assertive verb?\n",
    "report={}\n",
    "language={} #which language is the article from\n",
    "pages={} #length of the page\n",
    "start={} #starting point of the statement in the page\n",
    "pagelength={} #page length, this is for future use, if we want to track where the statement is placed in the page\n",
    "positives={}#statements with citation\n",
    "negatives={}#statements without citation\n",
    "text={}#raw text\n",
    "for lan in languages:\n",
    "    positives[lan]=[] #stores the statements needing a citation\n",
    "    negatives[lan]=[] #stores the statements without a citation (much less than the positives)\n",
    "    fraw=open(filenames_raw[lan]) #each line in fraw correspond to the line in f\n",
    "    #for each line in the vector file, record various parameters and then store the corresponding raw text with the same identifier\n",
    "    with open(filenames[lan]) as f:\n",
    "        for line in f:\n",
    "            unique=hashlib.sha224(line).hexdigest() #unique identifier of this line\n",
    "            #first, we store the raw statement text from the raw file\n",
    "            lineraw=fraw.readline() #line with raw text\n",
    "            rowraw=lineraw[:-1].split('\\t')\n",
    "            text[unique]=rowraw[-2] #where the text is placed in the line\n",
    "            \n",
    "            #now, we can get features\n",
    "            row=line.split('\\t')\n",
    "            labels[unique]=int(row[-2])#where the label sits in the feature file\n",
    "            txt = unicode(rowraw[-2], errors='ignore')\n",
    "\n",
    "            #we need to pre-process the statement by tokenizing it and annotating with POS tags.\n",
    "            statement_pos_tags = tag_statement(lan, txt)\n",
    "\n",
    "            #first append to lists of positives and negatives depending on the label\n",
    "            if labels[unique]==1:\n",
    "                positives[lan].append(unique)\n",
    "            else:\n",
    "                negatives[lan].append(unique)\n",
    "            #store features\n",
    "            vectors[unique]=[float(r) for r in row[-1].split(',')]\n",
    "            main[unique]= 1 if row[4]=='MAIN_SECTION'else 0\n",
    "            \n",
    "            #add the dictionary based features (as boolean flags)            \n",
    "            factive[unique] = annotate_line_with_dict_entries(lx_factive_verbs, statement_pos_tags, 'V')\n",
    "            implicative[unique] = annotate_line_with_dict_entries(lx_implicative_verbs, statement_pos_tags, 'V')\n",
    "            report[unique] = annotate_line_with_dict_entries(lx_report_verbs, statement_pos_tags, 'V')\n",
    "            hedges[unique] = annotate_line_with_dict_entries(lx_hedges, statement_pos_tags, '')\n",
    "            assertive[unique] = annotate_line_with_dict_entries(lx_assertive_verbs, statement_pos_tags, 'V')\n",
    "\n",
    "            language[unique]=lan\n",
    "            pages[unique]=int(row[0])\n",
    "            beginning=int(row[5])\n",
    "            offset=int(row[6])\n",
    "            l=beginning+offset\n",
    "            try:\n",
    "                base=pagelength[row[0]]\n",
    "                pagelength[row[0]]=l if l>base else base\n",
    "            except:\n",
    "                pagelength[row[0]]=l\n",
    "            start[unique]=beginning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allneg=count_negatives(negatives,positives)\n",
    "print allneg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print set(factive.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now select the data for training: all negatives + an equal number of positives, using only feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# factive implicative report hedges assertive\n",
    "\n",
    "print('all')\n",
    "y,values,ids=get_values_for_crossvalidation(positives,negatives,[factive,implicative,report,hedges,assertive])\n",
    "max_ind,max_val,scores=perform_gridsearch_withRFC(values,y)\n",
    "print('all+main')\n",
    "y,values,ids=get_values_for_crossvalidation(positives,negatives,[factive,implicative,report,hedges,assertive,main])\n",
    "max_ind,max_val,scores=perform_gridsearch_withRFC(values,y)\n",
    "print('all+main+vectors')\n",
    "y,values,ids=get_values_for_crossvalidation(positives,negatives,[factive,implicative,report,hedges,assertive,main,vectors])\n",
    "max_ind,max_val,scores=perform_gridsearch_withRFC(values,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now run a grid search to find the good random forest parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_ind,max_val,scores=perform_gridsearch_withRFC(values,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_train, val_test, y_train, y_test, ids_train, ids_test = train_test_split(values, y, ids, test_size=TEST_SIZE, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf,prob=train_test_final(val_train,val_test,y_train,50,200)\n",
    "print_top_bottom_sentences(prob,ids_test,y_test,text,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now lok at the effect of adding the 'main' features, i.e. a feature = 1 if the sentence is in the main section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_m,values_m,ids_m=get_values_for_crossvalidation(positives,negatives,[vectors,main])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_ind,max_val,scores=perform_gridsearch_withRFC(values_m,y_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_train, val_test, y_train, y_test, ids_train, ids_test = train_test_split(values, y, ids, test_size=TEST_SIZE, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf,prob=train_test_final(val_train,val_test,y_train,100,200)\n",
    "print_top_bottom_sentences(prob,ids_test,y_test,text,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
