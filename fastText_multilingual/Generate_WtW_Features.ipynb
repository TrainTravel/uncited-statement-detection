{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate words to watch multilingual features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "First, let's define a few simple functions... (from https://github.com/Babylonpartners/fastText_multilingual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from fasttext import FastVector\n",
    "\n",
    "languages=['en','fr','it']\n",
    "language_extended=['english','french','italian']\n",
    "\"\"\"\n",
    "to use this, you will need: \n",
    "1) alignment matrices from https://github.com/Babylonpartners/fastText_multilingual - place in alignemnt_matrices/\n",
    "2) Vectors from https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md - place in vectors/\n",
    "\"\"\"\n",
    "\n",
    "matrix_dir='alignment_matrices/'\n",
    "dic_dir='vectors/wiki.'\n",
    "rawdir='../data_clean/all_'\n",
    "feadir='features/all_ww_'\n",
    "infile='translations.tsv'\n",
    "\n",
    "dictionary={}\n",
    "filenames={}\n",
    "outfiles={}\n",
    "words={}\n",
    "# from https://stackoverflow.com/questions/21030391/how-to-normalize-array-numpy\n",
    "def normalized(a, axis=-1, order=2):\n",
    "    \"\"\"Utility function to normalize the rows of a numpy array.\"\"\"\n",
    "    l2 = np.atleast_1d(np.linalg.norm(a, order, axis))\n",
    "    l2[l2==0] = 1\n",
    "    return a / np.expand_dims(l2, axis)\n",
    "\n",
    "def make_training_matrices(source_dictionary, target_dictionary, bilingual_dictionary):\n",
    "    \"\"\"\n",
    "    Source and target dictionaries are the FastVector objects of\n",
    "    source/target languages. bilingual_dictionary is a list of \n",
    "    translation pair tuples [(source_word, target_word), ...].\n",
    "    \"\"\"\n",
    "    source_matrix = []\n",
    "    target_matrix = []\n",
    "\n",
    "    for (source, target) in bilingual_dictionary:\n",
    "        if source in source_dictionary and target in target_dictionary:\n",
    "            source_matrix.append(source_dictionary[source])\n",
    "            target_matrix.append(target_dictionary[target])\n",
    "\n",
    "    # return training matrices\n",
    "    return np.array(source_matrix), np.array(target_matrix)\n",
    "\n",
    "def learn_transformation(source_matrix, target_matrix, normalize_vectors=True):\n",
    "    \"\"\"\n",
    "    Source and target matrices are numpy arrays, shape\n",
    "    (dictionary_length, embedding_dimension). These contain paired\n",
    "    word vectors from the bilingual dictionary.\n",
    "    \"\"\"\n",
    "    # optionally normalize the training vectors\n",
    "    if normalize_vectors:\n",
    "        source_matrix = normalized(source_matrix)\n",
    "        target_matrix = normalized(target_matrix)\n",
    "\n",
    "    # perform the SVD\n",
    "    product = np.matmul(source_matrix.transpose(), target_matrix)\n",
    "    U, s, V = np.linalg.svd(product)\n",
    "\n",
    "    # return orthogonal transformation which aligns source language to the target\n",
    "    return np.matmul(U, V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to load filenames and word vectors. Non-english vectors are aligned to english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_filenames():\n",
    "    for lan,lext in zip(languages,language_extended):\n",
    "        #load clean data files\n",
    "        filenames[lan]=rawdir+lext+'.tsv'\n",
    "        #load output feature files\n",
    "        outfiles[lan]=feadir+lan+'.tsv'\n",
    "\n",
    "def load_dictionaries():\n",
    "    for lan in languages:\n",
    "        #load word vector dictionaries\n",
    "        dictionary[lan]= FastVector(vector_file=dic_dir+lan+'.vec')\n",
    "        #aligning all vectors to engglish\n",
    "        if lan !='en':\n",
    "            dictionary[lan].apply_transform(matrix_dir+lan+'.txt')\n",
    "            \n",
    "def load_words():\n",
    "    raw_words=[]\n",
    "    with open(infile,'rU') as f:\n",
    "        for line in f:\n",
    "            row=line[:-1].split('\\t')\n",
    "            raw_words.append(row[0])\n",
    "    for lan in languages:\n",
    "        words[lan]=[]\n",
    "    for w in raw_words:\n",
    "        for lan in languages:\n",
    "            if lan=='en':\n",
    "                words[lan].append(dictionary[lan][w])\n",
    "            else:\n",
    "                words[lan].append(dictionary[lan][dictionary[lan].translate_nearest_neighbour(dictionary['en'][w])])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we represent sentences with the algined vectos, and save the word representations in output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading word vectors from vectors/wiki.en.vec\n",
      "reading word vectors from vectors/wiki.fr.vec\n",
      "reading word vectors from vectors/wiki.it.vec\n",
      "7225\n",
      "9948\n",
      "5288\n",
      "154\n",
      "28464\n",
      "59987\n",
      "10358\n",
      "89121\n",
      "31870\n",
      "45996\n",
      "3051\n",
      "6576\n",
      "10661\n",
      "357622\n",
      "10607\n",
      "7502\n",
      "16846\n",
      "5207\n",
      "33203\n",
      "13212\n",
      "7043\n",
      "422539\n",
      "8567\n",
      "5256\n",
      "6793\n",
      "569057\n",
      "1172\n",
      "1587\n",
      "14091\n",
      "18829\n",
      "4045\n",
      "8080\n",
      "7816\n",
      "9443\n",
      "12368\n",
      "15399\n",
      "14593\n",
      "3283\n",
      "24264\n",
      "862502\n",
      "11722\n",
      "6576\n",
      "20773\n",
      "837565\n",
      "13194\n",
      "13406\n",
      "1960\n",
      "1931\n",
      "20200\n",
      "28605\n",
      "128944\n",
      "82366\n",
      "11535\n",
      "11756\n",
      "69902\n",
      "58994\n",
      "31529\n",
      "28408\n",
      "43530\n",
      "48850\n",
      "51448\n",
      "113235\n",
      "9555\n",
      "24597\n",
      "467808\n",
      "111692\n",
      "6183\n",
      "4350\n",
      "4728\n",
      "11495\n",
      "15125\n",
      "13956\n",
      "9508\n",
      "9904\n",
      "5547\n",
      "9220\n",
      "9508\n",
      "9904\n",
      "3336\n",
      "5035\n",
      "1137\n",
      "996\n",
      "529\n",
      "714520\n",
      "5575\n",
      "5493\n",
      "26103\n",
      "23027\n",
      "76874\n",
      "15797\n",
      "4173\n",
      "15797\n",
      "12652\n",
      "17585\n",
      "8362\n",
      "8340\n",
      "13690\n",
      "13926\n",
      "16140\n",
      "20690\n",
      "22622\n",
      "17380\n",
      "25208\n",
      "732\n",
      "52\n"
     ]
    }
   ],
   "source": [
    "#first load variables and dictionaries\n",
    "load_filenames()\n",
    "load_dictionaries()\n",
    "load_words()\n",
    "l=len(words['en'])\n",
    "\n",
    "print l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for every language, generate aligned vectors for clean sentences and write to file\n",
    "for lan in languages:\n",
    "    #open outfile for writing\n",
    "    fo=open(outfiles[lan],'w')\n",
    "    with open(filenames[lan]) as f:\n",
    "        #for every sentence in the clean filename\n",
    "        for line in f:\n",
    "            #isolate the text\n",
    "            row=line[:-1].split('\\t')\n",
    "            text=row[-2]\n",
    "            #split into words\n",
    "            rowwords=text.split()\n",
    "            #populate vector with sum of word vectors\n",
    "            outvec=np.zeros(l)\n",
    "            count=0\n",
    "            for w in rowwords:\n",
    "                try:\n",
    "                    vec=dictionary[lan][w]\n",
    "                    for i in enumerate(words[lan]):\n",
    "                            outvec[i[0]]+=FastVector.cosine_similarity(vec,i[1])\n",
    "                            count=count+1      \n",
    "                except:\n",
    "                    try:\n",
    "                        vec=dictionary[lan][w.lower()]\n",
    "                        for i in enumerate(words[lan]):\n",
    "                            outvec[i[0]]+=FastVector.cosine_similarity(vec,i[1])\n",
    "                            count=count+1\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "            #divide by the total number of matching vetors\n",
    "            if count>0:\n",
    "                outvec /=count\n",
    "            else:\n",
    "                outvec=np.ones(l)\n",
    "            outvec[outvec == np.nan] =0\n",
    "            outvec[outvec == np.inf] = 0\n",
    "            outvec[outvec == -np.inf] = 0\n",
    "            #build a comma-separated string for the sentence vectors\n",
    "            out=','.join([str(c) for c in outvec])\n",
    "            #rebuild output string\n",
    "            outstring='\\t'.join(row[:-2])+'\\t'+row[-1]+'\\t'+out+'\\n'\n",
    "            #writes to file\n",
    "            fo.write(outstring)\n",
    "    fo.close()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
